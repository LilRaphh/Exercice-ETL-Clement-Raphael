# Projet Int√©gr√© : √âvaluation de la Cor√©lation entre la m√©t√©o et le passage dans les rues de Rennes

## Technologies Utilis√©es

### Langage

![Python](https://img.shields.io/badge/Python-3.12.7-blue?logo=python&logoColor=white)

### Framework et outils de d√©veloppement

![Docker](https://img.shields.io/badge/Docker-20.10.7-blue?logo=docker&logoColor=white)


### Biblioth√®ques de Donn√©es & Machine Learning

![Pandas](https://img.shields.io/badge/Pandas-1.5.2-brightgreen?logo=pandas&logoColor=white)
![MySQL](https://img.shields.io/badge/MySQL-8.4-brightgreen?logo=mysql&logoColor=white)

### Outils de Visualisation

![Apache Superset Badge](https://img.shields.io/badge/Apache%20Superset-4.1.0-brightgreen?logo=apachesuperset&logoColor=white)

## Introduction au Projet
---

Ces outils ont √©t√© utilis√© pour le developpement du Projet Exercice-ETL-Clement-Raphael. Ce dernier vise √† r√©cup√©rer des donn√©es provenant de diverses sources, comme une API et un site internet, afin de de les transformer, pour ensuite les analyser. Tout cela de mani√®re √† pouvoir en tirer des insights et potentiellement, par la suite, en faire des pr√©dictions.

## Objectif du Projet 
---
Ce projet vise √† combiner les donn√©es d'un site m√©t√©o (infoclimat) ainsi que les donn√©es fournies par une API Rennes Metropoles. Cete d√©marche permet de pouvoir d√©t√©cter des biais de comportements, que ce soit vis-√†-vis de la m√©t√©o mais aussi de p√©riodes de l'ann√©es, ou des √©v√©nements. Et cela afin de pouvoir au long terme donner des voies d'am√©lioration quant √† la circulation au sein de la m√©tropole de Rennes.

## Architecture du Projet
---

<img width="725" alt="Capture d‚Äô√©cran 2024-11-15 √† 15 15 31" src="https://github.com/user-attachments/assets/a07471d4-3756-4139-9e65-8cde17b39086">

## Workflow et sch√©ma d'architecture

1. **R√©cup√©ration des donn√©es (API et scrapping)** :

  - Exctration des donn√©es √† partir de la page m√©t√©o, √† l'aide d'un script Python.
  - R√©cup√©ration des donn√©es de l'API pour les transformer sous le m√™me format que le scrapping.

2. **Traitement des donn√©es**

  - **Transformation des donn√©es / Simplification :** Une fois la r√©cup√©ration de toutes les donn√©es, √† travers le scrapping de toutes les pages depuis le 1er janvier 2023, nettouage de toutes les donn√©es √† l'aide d'un second script python.
  - **merging des donn√©es :** merging des deux sources des donn√©es par la date.

3. **Envoie des donn√©es vers la base de donn√©e**

  - Envoie des donn√©es vers la base de donn√©es mySql √† partir de scripts Python.

4. **Visualisation et Analyse**

  - Superset r√©cup√©re les donn√©es pour cr√©er une visualisation

# **Guide d'Installation**

## **Pr√©requis**
Avant de commencer, assurez-vous d'avoir les outils suivants install√©s sur votre machine :
1. **Python 3.12.7** ou une version compatible.
   - V√©rifiez en ex√©cutant : `python --version` ou `python3 --version`.
2. **Docker** (version 20.10.7 ou ult√©rieure).
   - V√©rifiez en ex√©cutant : `docker --version`.
3. **Git** pour cloner le d√©p√¥t.
   - V√©rifiez en ex√©cutant : `git --version`.

---

## **√âtape 1 : Cloner le d√©p√¥t**
T√©l√©chargez le code source de ce projet depuis le d√©p√¥t GitHub :
```bash

git clone https://github.com/votre-utilisateur/Exercice-ETL-Clement-Raphael.git

cd Exercice-ETL-Clement-Raphael
```
---

## **√âtape 2 : Configurer l'environnement Python**

1. **Cr√©er un environnement virtuel et l'activer** (recommand√© pour isoler les d√©pendances du projet) :
   - Sur Unix/macOS :
     ```bash
     python3 -m venv venv
     source venv/bin/activate
     ```
   - Sur Windows :
     ```bash
     python -m venv venv
     venv\Scripts\activate
     ```

2. **Mettre √† jour `pip` et installer les d√©pendances** :
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```
---

## **√âtape 3 : Configurer Docker pour les services**
Certains services, comme les bases de donn√©es ou les outils de visualisation, peuvent √™tre ex√©cut√©s √† l'aide de Docker.

1. **Aller dans le bon dossier** (si applicable) :
   - Le premier docker √† lancer est le docker compose qui est dans le dossier data engeneering. Celui ci va up la base de donn√©e.
   - Exemple :
     ```bash
      cd data_engeneering     
     ```

2. **Lancer Docker** :
   ```bash
   docker-compose up --build -d
   ```

## **√âtape 4 : Initialiser le superset (la visualisation)**

Pour la visualisation nous utilisons superset. 

**Aller dans le dossier racine du projet**

**Lancer la commande get_superset_repo.sh**
Cela va cloner le projet superset dans un dossier DataViz

   ```bash
    .\get_superset_repo.sh 
   ```

**Lancer le docker superset**
Une fois le clone du repo fini aller dans le dossier dataViz

  ```bash
  cd DataViz
  ```

Maintenant lancer le docker compose de notre superset.

  ```bash
  docker compose -f docker-compose-non-dev.yml up -d
  ```

## **√âtape 5 : Initialiser les donn√©es dans la base de donn√©e**

1. **Lancer le script python**:

Pour inserer les donn√©es dans la base de donn√©e, un script est fais pour cela. Ce script va aller scraper et r√©cup√©rer les donn√©es.

Pour lancer ce script, v√©rifier que vous etes bien dans l'environnement virtuel 

![image](https://github.com/user-attachments/assets/0a0567de-1d69-4af9-80ae-2477a695d8a6)

Ici on peut voir qu'on est dans notre environnement virtuel grace √† notre 'env'

**Allez dans le dossier ETL**:

  ```bash
  cd ETL
  ```

**Lancer le script**

  ```bash
  python init_db_etl.py
  ```

## **√âtape 6 : V√©rification des donn√©es dans la base de donn√©e **

Pour cela nous avons une interface web minimaliste qui est Adminer. 

**Se rendre sur le site**
Pour se rendre sur l'interface web il faut aller sur l'url suivante: 

```http://<ip-de-ma-machine>:8080/ ```

Une fois sur l'interface web, si votre contenaire docker est bien run (Etape 3)
vous devriez arriver sur cette page:

![image](https://github.com/user-attachments/assets/89ef9366-5048-4ce0-835b-aa025bae6d27)

Les identifiants de connexion sont les suivants: 

- Syst√®me         : MySQL
- server          : db
- utilisateur     : root
- Mot de passe    : admin
- Base de donn√©es : ETL


=======
# üìú Conclusion

---

L‚Äôapplication d√©velopp√©e dans le cadre de ce projet met en lumi√®re l‚Äôinteraction entre les conditions m√©t√©orologiques et les flux de passage en diff√©rents points de la ville de Rennes. Gr√¢ce √† l‚Äôexploitation combin√©e de donn√©es issues d‚Äôun site m√©t√©o scrapp√© et d‚Äôune API d√©di√©e aux flux de passage, nous avons pu √©tablir des liens de corr√©lation int√©ressants tout au long de l‚Äôann√©e.

En s‚Äôappuyant sur des techniques de web scraping et d‚Äôint√©gration d‚ÄôAPI, l‚Äôapplication offre une analyse approfondie et quantifi√©e de la mani√®re dont les variations climatiques influencent les d√©placements urbains. Ces r√©sultats peuvent fournir des insights pr√©cieux pour les gestionnaires urbains, les commer√ßants, ou toute entit√© cherchant √† anticiper les comportements li√©s aux flux de population en fonction des conditions climatiques.

Les visualisations g√©n√©r√©es permettent d‚Äôexplorer ces corr√©lations de mani√®re intuitive et interactive, ouvrant la voie √† une meilleure compr√©hension des dynamiques urbaines. Ce projet pourrait √©galement servir de base pour des d√©veloppements futurs, tels que l‚Äôint√©gration de mod√®les pr√©dictifs ou l‚Äôanalyse de flux dans d‚Äôautres contextes g√©ographiques.

En conclusion, cette solution d√©montre comment des donn√©es disparates peuvent √™tre combin√©es et analys√©es pour r√©v√©ler des tendances utiles et exploitables. Elle repr√©sente une avanc√©e vers une gestion des flux urbains plus proactive, tout en soulignant le r√¥le cl√© des conditions environnementales dans la planification et la prise de d√©cision.

üöß Difficult√©s Rencontr√©es

  texte


## Am√©lioration future
---
  


## Contributeurs

  - Cl√©ment Metois (@Skyane) : Apprenti Data Scientist -**skayne.pro@gmail.com**-
  - Colnot Rapha√´l (@LilRaphh) : Apprenti Data Scientist -**colnotraphael@gmail.com**-


## Licence

Ce projet a √©t√© r√©aslis√© pour rendu en √©cole.









